1.python 기초문법

2.크롤링 개념

3.크롤링 기초
- requests
- bs4

4.크롤링 중급
- selenium
> XPATH : 셀레니움을 활용해서 크롤링을 해오는 방법중 하나
> 아이디, 태그이름, CSS 선택자
> json, xml
> 하나의 웹사이트 개발 > 언어 종류는 한정적이나, 해당 언어를 어떤 방식으로 컴파일링 할 것인가에 대한 선택은 엄청 다양
> html, css, js
> TS, React, Vue, Angluar, Pyscript, Ruby, Next, Nest…..
> 아나콘다 > 주피터노트북 > gui

- scrapy
> 크롤링 전문 프레임워크 (VS 라이브러리)
> 현업에서 대량의 데이터를 크롤링 해와야하는 경우
> 안정적 : 디버깅이 강하다 (*오류 -> 명확하게 알려줌)
> 속도 : requests < selenium < scrapy (*비동기방식으로 진행됨)
> 터미널 > 명령어를 자주 입력 cli

> vs code 설치 필수
> 아나콘다, 주피터노트북 (*컴파일러) & vs code (*웹 에디터) 연동

- 특정 웹사이트 > 복수의 이미지를 크롤링 해오는 방법 연습

- 절대경로, 상대경로

강사 맥북 노트북에서는 주피터 노트북 프로그램 어디를 루트 = 뿌리
> 	admin

상대경로

> 상대경로 // 실무
> 윈도우 & 맥 경로 표기방법
> /Users/admin/pythonBasic_v1/selenium/250724
> .. : 한 단계 위로
> . : 내가 위치한 폴더
> Operating System 

크롤링하고자 하는 문서에서 이미지 파일의 경로를 가지고 있는 태그요소를 먼저찾아오기

해당 태그 요소를 찾았다면, 속성값에서 이미지 경로 url을 찾아오기

파일명.확장자 (*jpg, jpeg, png, svg)

해당 url 문자열을 구분 

https://divjason.github.io/sellenium-test/
skill-name 클래스 값을 가지고 있는 div 태그를 찾아서 출력!!!

블로그 사이트 > p태그를 가지고 있는 요소를 찾아오세요. 단, 어떤 속성이던 상관없으나, 일단 속성을 가지고 있는 p태그를 찾아오세요!

class 속성값이 news 인 요소를 찾아오시고, 텍스트 값만 출력!!

1. xpath 활용!!!

2. 아이디 : error@error.com 입력

3. 비밀번호 : 1234 입력

4. 로그인 버튼 클릭

5. 로그인 페이지에서 message 클래스 속성값의 텍스트를 출력!

https://divjason.github.io/sellenium-test/index.html
https://divjason.github.io/sellenium-test/index_login.html

1.스크래치 설치 : pip install scrapy

2.우회, 설치 : python -m pip install scrapy

3.스크래치 프로젝트 세팅 : scrapy startproject 프로젝트명

4.scrapy.cfg : 크롤링 이후 수집해온 데이터를 어떻게 서버에 배포할 것인지를 설정해놓은 파일

5.helloscrapy : 프로젝트 폴더명

6.__init__.py : 현재 프로젝트 폴더를 파이썬 패키지로 만들어주는 초기 역할 > 스크래피도 기존 파이썬 문법처럼 이미 누군가가 만들어놓은 라이브러리를 가져다 사용가능합니다!!! 

7.items.py : 프로젝트 내 아이템 정의 파일. 크롤링을 해오는데 예를 들어 A데이터 찾아오기 / B데이터 찾아오기 / C데이터 찾아오기 => 크롤링을 하기 위한 각각의 실행 명령 아이템을 사전에 정의해놓는 파일

8.pipelines.py : 앞에서 정의해놓은 실행 명령 아이템들을 어떤 순서 및 방법으로 처리할 것인지 설정 해놓은 파일

9.settings.py : 프로젝트 배포를 위한 실제 설정 파일. 스크래피를 통해서 크롤링할 때, 동시에 실행할 크롤링 수는 몇개, 미들웨어는 어떤것을 사용할지, 기타 세부적인 설정

10.spiders : 실제 크롤링을 하는 역할 > 크롤러(*스파이더) > 외부 라이브러리를 가져다가 사용할 수 있게끔 하기 위해서 > __init__.py 존재

11.middlewares.py : 비동기적으로 실행되는 프로그래밍 언어들에 존재하는 개념적인 요소 -> 주로 최종 데이터 집계 전 각각의 아이템들의 정보를 수집한 후 최종적인 연산처리를 하는데 사용됨

ls : list : 내가 현재 위치한 폴더 확인
dir : directory : 내가 현재 위치한 폴더 확인
pwd : print working directory : 현재 내가 작업중인 폴더 경로 확인

cd helloscrapy

크롤러 생성 :
scrapy genspider <크롤러이름> <크롤링할 페이지 주소>

scrapy genspider davidfun https://davelee-fun.github.io

----

아이템스 설정 // 파이프라인 세팅

----

크롤러를 활용해서 크롤링을 해오라고 하는 명령어
scrapy crawl <크롤러이름>
scrapy crawl davidfun

완료된 크롤링 데이터를 저장하도록 하는 명령어
scrapy crawl davidfun -o results.json
scrapy crawl davidfun -o results.xml
scrapy crawl davidfun -o results.csv
(*-o : output -> 기본적으로 소문자 실행, -O -> 기존 데이터에서 덮어쓰기)

실제 크롤링을 담당하는 크롤러 (*스파이더)는 spiders > davidfun.py라는 파일안에 클래스로 정의!!!!

클래스는 scrapy.Spider (*스크래피의 창조주 = 개발자) 라는 기본값을 상속받아서 생성됨

name : 크롤러의 이름
allowed_domains : 	크롤링해올 사이트의 도메인 리스트 정의
start_urls : 크롤링을 시작할 메인 URL 리스트를 지정

크롤러에게 본격적으로 크롤링해야할 대상, 방법을 설정!!!

response :
- response.css(‘selector::text’).get() : css 선택자를 사용해서 텍스트를 추출하는 방식

- response.xpath(‘xpath’).get() : xpath를 사용해서 요소를 선택하고 텍스트를 추출하는 방식

- get( ) : 선택한 요소의 첫 번째 결과를 찾으면 해당 결과값을 반환. 종료. 찾아올 결과값이 없으면 none 을 반환

- getall( ) : 선택한 모든 요소를 반환. 만약 값이 없다면 빈 “리스트”

cd ..

scrapy startproject davidfun_project1

cd davidfun_project1

scrapy genspider davidfun davelee-fun.github.io




